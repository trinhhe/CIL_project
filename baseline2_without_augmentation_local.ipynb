{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e534e48711db4d1e1c48977d0d14ff85b1f16d41bcc4fdfd88268a329b3c9d66"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/elhamaminmansour/Library/Python/2.7/lib/python/site-packages (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/elhamaminmansour/Library/Python/2.7/lib/python/site-packages (from pandas) (1.16.6)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /Users/elhamaminmansour/Library/Python/2.7/lib/python/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2011k in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from pandas) (2013.7)\n",
      "Requirement already satisfied: six>=1.5 in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3.4 is available.\n",
      "You should consider upgrading via the '/System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Python 3.7.6\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n    To execute the code run the following command\\n        python baseline2.py\\n\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from PIL import Image\n",
    "import config\n",
    "# importing the libraries\n",
    "!pip2 install pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# for reading and displaying images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# for creating validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# for evaluating the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "!python3 --version\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "\"\"\"\n",
    "    To execute the code run the following command\n",
    "        python baseline2.py\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.5142, 0.1984, 0.6996],\n        [0.8124, 0.8942, 0.0901],\n        [0.1650, 0.8919, 0.0785],\n        [0.1521, 0.3721, 0.5169],\n        [0.4434, 0.1126, 0.8209]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(infilename):\n",
    "    data = mpimg.imread(infilename)\n",
    "    return data\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - np.min(img)\n",
    "    rimg = (rimg / np.max(rimg) * 255).round().astype(np.uint8)\n",
    "    return rimg\n",
    "\n",
    "# Concatenate an image and its groundtruth\n",
    "def concatenate_images(img, gt_img):\n",
    "    nChannels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if nChannels == 3:\n",
    "        cimg = np.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = np.zeros((w, h, 3), dtype=np.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)\n",
    "        gt_img_3c[:,:,0] = gt_img8\n",
    "        gt_img_3c[:,:,1] = gt_img8\n",
    "        gt_img_3c[:,:,2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = np.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading 100 images\n",
      "Loading 100 images\n",
      "satImage_052.png\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(config.ground_truth_PATH_TRAIN)\n",
    "n = len(files) # Load maximum 20 images\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = [load_image(os.path.join(config.images_PATH_TRAIN, files[i])) for i in range(n) if files[i].endswith('.png')]\n",
    "\n",
    "#gt_dir = os.path.join(data_dir,  ground_truth_PATH_TRAIN)\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = [load_image(os.path.join(config.ground_truth_PATH_TRAIN, files[i])) for i in range(n) if files[i].endswith('.png')]\n",
    "print(files[0])\n",
    "\n",
    "n = 100 # Only use 10 images for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "img_patches:  (100, 625, 16, 16, 3)\nimg_patches:  (62500, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "#print('Image size = ' + str(imgs[0].shape[0]) + ',' + str(imgs[0].shape[1]))\n",
    "# Extract patches from input images\n",
    "patch_size = 16 # each patch is 16*16 pixels\n",
    "\n",
    "img_patches = [img_crop(imgs[i], patch_size, patch_size) for i in range(n)]\n",
    "gt_patches = [img_crop(gt_imgs[i], patch_size, patch_size) for i in range(n)]\n",
    "\n",
    "\n",
    "print(\"img_patches: \", np.array(img_patches).shape )\n",
    "# Linearize list of patches\n",
    "img_patches = np.asarray([img_patches[i][j][:,:,0] for i in range(len(img_patches)) for j in range(len(img_patches[i]))])\n",
    "print(\"img_patches: \", img_patches.shape)\n",
    "gt_patches =  np.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 6-dimensional features consisting of average RGB color as well as variance\n",
    "def extract_features(img):\n",
    "    feat_m = np.mean(img, axis=(0,1))\n",
    "    feat_v = np.var(img, axis=(0,1))\n",
    "    feat = np.append(feat_m, feat_v)\n",
    "    return feat\n",
    "\n",
    "# Extract 2-dimensional features consisting of average gray color as well as variance\n",
    "def extract_features_2d(img):\n",
    "    #print(\"small image dim: \", img.shape)\n",
    "    feat_m = np.mean(img)\n",
    "    feat_v = np.var(img)\n",
    "    feat = np.append(feat_m, feat_v)\n",
    "    return feat\n",
    "\n",
    "# Extract features for a given image\n",
    "def extract_img_features(filename):\n",
    "    img = load_image(filename)\n",
    "    #print(\"img size: \", img.shape)\n",
    "    img_patches = img_crop(img, patch_size, patch_size)\n",
    "    X = np.asarray([extract_features_2d(img_patches[i]) for i in range(len(img_patches))])\n",
    "    return X\n",
    "\n",
    "\n",
    "# Compute features for each image patch\n",
    "foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_to_class(v):\n",
    "    df = np.sum(v)\n",
    "    #print(\"df: \", df)\n",
    "    if df > foreground_threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#X = np.asarray([extract_features_2d(img_patches[i]) for i in range(len(img_patches))])\n",
    "X = img_patches \n",
    "#print(\"the shape of X: \", X.shape)\n",
    "Y = np.asarray([value_to_class(np.mean(gt_patches[i])) for i in range(len(gt_patches))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape of X:  (62500, 1, 16, 16)\nshape of Y:  (62500,)\n"
     ]
    }
   ],
   "source": [
    "X2= X.reshape(62500, 1, 16, 16)\n",
    "print(\"shape of X: \", X2.shape)\n",
    "\n",
    "print(\"shape of Y: \", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([62500, 1, 16, 16]), torch.Size([62500]))"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "\n",
    "train_x  = torch.from_numpy(X2)\n",
    "\n",
    "# converting the target into torch format\n",
    "train_y =  Y.astype(int)\n",
    "train_y = torch.from_numpy(Y)\n",
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):   \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.cnn_layers = Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            Conv2d(1, 4, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(4),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(4),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        print(x.size())\n",
    "        x = self.linear_layers(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (cnn_layers): Sequential(\n    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): ReLU(inplace=True)\n    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (linear_layers): Sequential(\n    (0): Linear(in_features=64, out_features=10, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "# defining the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.07)\n",
    "# defining the loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "        \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    # getting the training set\n",
    "    x_train, y_train = Variable(train_x), Variable(train_y)\n",
    "    # getting the validation set\n",
    "    x_val, y_val = Variable(train_x), Variable(train_y)\n",
    "    # converting the data into GPU format\n",
    "    if torch.cuda.is_available():\n",
    "        x_train = x_train.cuda()\n",
    "        y_train = y_train.cuda()\n",
    "        x_val = x_val.cuda()\n",
    "        y_val = y_val.cuda()\n",
    "\n",
    "    # clearing the Gradients of the model parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # prediction for training and validation set\n",
    "    output_train = model(x_train)\n",
    "    output_val = model(x_val)\n",
    "\n",
    "    # computing the training and validation loss\n",
    "    loss_train = criterion(output_train, y_train)\n",
    "    loss_val = criterion(output_val, y_val)\n",
    "    train_losses.append(loss_train)\n",
    "    val_losses.append(loss_val)\n",
    "\n",
    "    # computing the updated weights of all the model parameters\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    tr_loss = loss_train.item()\n",
    "    if epoch%2 == 0:\n",
    "        # printing the validation loss\n",
    "        print('Epoch : ',epoch+1, '\\t', 'loss :', loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "Epoch :  1 \t loss : tensor(3.0018, grad_fn=<NllLossBackward>)\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "Epoch :  3 \t loss : tensor(0.6199, grad_fn=<NllLossBackward>)\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "Epoch :  5 \t loss : tensor(0.5771, grad_fn=<NllLossBackward>)\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "Epoch :  7 \t loss : tensor(0.5790, grad_fn=<NllLossBackward>)\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "Epoch :  9 \t loss : tensor(0.5416, grad_fn=<NllLossBackward>)\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "Epoch :  11 \t loss : tensor(0.5534, grad_fn=<NllLossBackward>)\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "Epoch :  13 \t loss : tensor(0.5394, grad_fn=<NllLossBackward>)\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "Epoch :  15 \t loss : tensor(0.5387, grad_fn=<NllLossBackward>)\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "Epoch :  17 \t loss : tensor(0.5316, grad_fn=<NllLossBackward>)\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "Epoch :  19 \t loss : tensor(0.5299, grad_fn=<NllLossBackward>)\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "Epoch :  21 \t loss : tensor(0.5311, grad_fn=<NllLossBackward>)\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "Epoch :  23 \t loss : tensor(0.5240, grad_fn=<NllLossBackward>)\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "torch.Size([62500, 64])\n",
      "Epoch :  25 \t loss : tensor(0.5254, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# defining the number of epochs\n",
    "n_epochs = 25\n",
    "# empty list to store training losses\n",
    "train_losses = []\n",
    "# empty list to store validation losses\n",
    "val_losses = []\n",
    "# training the model\n",
    "for epoch in range(n_epochs):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-739420dcfb73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# prediction for training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msoftmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             raise AssertionError(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# prediction for training set\n",
    "with torch.no_grad():\n",
    "    output = model(train_x.cuda())\n",
    "    \n",
    "softmax = torch.exp(output).cpu()\n",
    "prob = list(softmax.numpy())\n",
    "predictions = np.argmax(prob, axis=1)\n",
    "\n",
    "# accuracy on training set\n",
    "accuracy_score(train_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}